{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf51f63c",
   "metadata": {},
   "source": [
    "# Build Docker Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934109e4",
   "metadata": {},
   "source": [
    "1. We should have **Docker Desktop** application installed in your system.\n",
    "\n",
    "- start the docker in desktop\n",
    "\n",
    "2. We should clone the code in any folder and open in vscode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c7757e",
   "metadata": {},
   "source": [
    "3. check the `docker --version`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1783310b",
   "metadata": {},
   "source": [
    "4. check the `docker images`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f496ddca",
   "metadata": {},
   "source": [
    "We have to create a **Docker image** along with **Airflow**, dockerize the source code in docker along with airflow, airflow doesn't work directly in *Windows*. it works well in linux environment and easy to setup, thats why will use *airflow* in docker.\n",
    "\n",
    "benifite of using docker is it isolates your underlying infrastructure with your application dependencies.\n",
    "\n",
    "What are your application dependencies ?\n",
    "\n",
    "- linux environment\n",
    "- python environment\n",
    "- requirement.txt file\n",
    "- Apache Airflow\n",
    "\n",
    "docker will Isolate this infrastructure from outside environment that is the benifite of having docker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc76f95a",
   "metadata": {},
   "source": [
    "So we have to create a **Dockerfile**\n",
    "\n",
    "```Dockerfile\n",
    "FROM python:3.8\n",
    "USER root\n",
    "RUN mkdir /app \n",
    "COPY . /app/\n",
    "WORKDIR /app/\n",
    "RUN pip3 install -r requirements.txt \n",
    "ENV AIRFLOW_HOME = \"/app/airflow\"\n",
    "ENV AIRFLOW__CORE__DAGBAG__IMPORT_TIMEOUT = 1000\n",
    "ENV AIRFLOW__CORE__ENABLE_XCOM_PICKING = True\n",
    "RUN airflow db init\n",
    "RUN airflow users create -e rahulshelke3399@gmail.com -f Rahul -l Shelke -p admin -r Admin -u admin\n",
    "\n",
    "RUN chmod 777 start.sh\n",
    "RUN apt update -y && apt install awscli -y\n",
    "ENTRYPOINT [\"/bin/sh\"]\n",
    "CMD [\"start.sh\"]\n",
    "```\n",
    "\n",
    "- `FROM python:3.8` : we will be using *python:3.8* as a base image.\n",
    "- `USER root` : accessing system as root user\n",
    "- `RUN mkdir /app` : creating a `app` directory in current directory\n",
    "- `COPY . /app/` : copy the `.` current directory files to `/app` folder\n",
    "- `WORKDIR /app/` : setting the working directory to `/app/`\n",
    "- `RUN pip3 install -r requirements.txt` : install requirement files, file will also install the `apache-airflow`\n",
    "    - if we have to use `apache-airflow` then we have to set `AIRFLOW_HOME` directory\n",
    "\n",
    "- `ENV AIRFLOW_HOME = \"/app/airflow\"` : will create `airflow` folder in `app` folder there will keep our airflow.\n",
    "\n",
    "- `ENV AIRFLOW__CORE__DAGBAG__IMPORT_TIMEOUT = 1000` : how much time we want it to wait, cause some times code takes time to execute. (almost 17-18 min)\n",
    "\n",
    "- `ENV AIRFLOW__CORE__ENABLE_XCOM_PICKING = True` : just consider that we have to pass some information from one component to another component, so there is possibility that we will have some object that will has to be serialized , for that we have to set this environment variable to `True`.\n",
    "\n",
    "    - In Airflow, XComs allow tasks to exchange small amounts of data. For example, Task A can push a value, and Task B can pull that value and use it.\n",
    "    - You can pass complex Python objects between tasks (e.g., NumPy arrays, Pandas DataFrames, or custom classes).\n",
    "    - The data is serialized using pickle, Pythonâ€™s object serialization module.\n",
    "\n",
    "- `RUN airflow db init` : airflow is a schedular , every schedular requires a database to keep information regarding users, to manage access heirarchy and to keep track of job running, which job ran when. and other related configuration. (by default it uses `SQLlite`, but we can use any)\n",
    "\n",
    "- `RUN airflow users create -e <user-email> -f <first-name> -l <last-name> -p <password> -r <role> -u <user-name>` : we are hard coding the variables , if we need to log in to airflow will require <password> and <user-name>.\n",
    "\n",
    "- `RUN chmod 777 start.sh` : change the permissions of the file start.sh so that anyone (owner, group, others) can **read, write,** and **execute** it.\n",
    "\n",
    "- `RUN apt update -y && apt install awscli -y` : we also going to use *AWS S3* buckt because of that we are installing aswcli\n",
    "\n",
    "- `ENTRYPOINT [\"/bin/sh\"]` : this a entry point to run bash script\n",
    " \n",
    "- `CMD [\"start.sh\"]` : this is a bash script to run\n",
    "\n",
    "\n",
    "By default, Airflow stores this data in the metadata database using JSON serialization.\n",
    "Airflow has two things to start airflow we have two things\n",
    "1. Dahsboard\n",
    "2. Airflow Scheduler\n",
    "\n",
    "to start this will write one bash script\n",
    "\n",
    "*start.sh* : it will going to contain some code, which can start our **Airflow Schedular** and **Airflow Web-Server** \n",
    "\n",
    "```bash\n",
    "#!bin/sh\n",
    "\n",
    "nohup airflow scheduler &\n",
    "airflow webserver\n",
    "```\n",
    "\n",
    "select end of line sequence $\\longrightarrow$ **LR**, only then this bash script will execute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624e3503",
   "metadata": {},
   "source": [
    "create docker image in terminal using following command\n",
    "\n",
    "`docker build -t <image-name>:<tag> .`\n",
    "\n",
    "docker image build will start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30453365",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
