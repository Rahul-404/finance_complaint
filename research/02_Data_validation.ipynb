{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/rahulshelke/Documents/Data-Science/Data-Science-Projects/finance_complaint/research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/rahulshelke/Documents/Data-Science/Data-Science-Projects/finance_complaint'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **schema.py**\n",
    "\n",
    "basically defining the schema for data , to put it into a tabular fromat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading environment variables\n",
      "Read Complete!\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "from pyspark.sql.types import (TimestampType, StringType, FloatType, StructType, StructField)\n",
    "from pyspark.sql import DataFrame\n",
    "from finance_complaint.exception import FinanceException\n",
    "from finance_complaint.logger import logging as logger\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinanceDataSchema:\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" \n",
    "        here we are going to set our data's column names\n",
    "        \"\"\"\n",
    "        self.col_company_response: str = 'company_response'\n",
    "        self.col_consumer_consent_provided: str = 'consumer_consent_provided'\n",
    "        self.col_submitted_via = 'submitted_via'\n",
    "        self.col_timely: str = 'timely'\n",
    "        self.col_diff_in_days: str = 'diff_in_days'\n",
    "        self.col_company: str = 'company'\n",
    "        self.col_issue: str = 'issue'\n",
    "        self.col_product: str = 'product'\n",
    "        self.col_state: str = 'state'\n",
    "        self.col_zip_code: str = 'zip_code'\n",
    "        self.col_consumer_disputed: str = 'consumer_disputed'\n",
    "        self.col_date_sent_to_company: str = 'date_sent_to_company'\n",
    "        self.col_date_received: str = 'date_received'\n",
    "        self.col_complaint_id: str = 'complaint_id'\n",
    "        self.col_sub_product: str = 'sub_product'\n",
    "        self.col_complaint_what_happened: str = 'complaint_what_happened'\n",
    "        self.col_company_public_response: str = 'company_public_response'\n",
    "\n",
    "    @property\n",
    "    def dataframe_schema(self) -> StructType:\n",
    "        \"\"\" \n",
    "        now specifying each columns data type\n",
    "        \"\"\"\n",
    "        try:\n",
    "            schema = StructType(\n",
    "                [\n",
    "                    StructField(self.col_company_response, StringType()),\n",
    "                    StructField(self.col_consumer_consent_provided , StringType()),\n",
    "                    StructField(self.col_submitted_via, StringType()),\n",
    "                    StructField(self.col_timely, StringType()),\n",
    "                    StructField(self.col_date_sent_to_company, TimestampType()),\n",
    "                    StructField(self.col_date_received, TimestampType()),\n",
    "                    StructField(self.col_company, StringType()),\n",
    "                    StructField(self.col_issue, StringType()),\n",
    "                    StructField(self.col_product, StringType()),\n",
    "                    StructField(self.col_state, StringType()),\n",
    "                    StructField(self.col_zip_code, StringType()),\n",
    "                    StructField(self.col_consumer_disputed, StringType()),\n",
    "                ]\n",
    "            )\n",
    "            return schema\n",
    "        except Exception as e:\n",
    "            raise FinanceDataSchema(e, sys)\n",
    "\n",
    "    @property\n",
    "    def target_column(self) -> str:\n",
    "        return self.col_consumer_disputed\n",
    "    \n",
    "    @property\n",
    "    def one_hot_encoding_features(self) -> List[str]:\n",
    "        fetaures = [\n",
    "            self.col_company_response,\n",
    "            self.col_consumer_consent_provided,\n",
    "            self.col_submitted_via,\n",
    "        ]\n",
    "        return fetaures\n",
    "    \n",
    "    @property\n",
    "    def tfidf_fetaures(self) -> List[str]:\n",
    "        fetaures = [\n",
    "            self.col_issue\n",
    "        ]\n",
    "        return fetaures\n",
    "    \n",
    "    @property\n",
    "    def required_columns(self) -> List[str]:\n",
    "        features = [self.target_column] + self.one_hot_encoding_features + self.tfidf_fetaures + \\\n",
    "                    [self.col_date_sent_to_company, self.col_date_received]\n",
    "        return features\n",
    "    \n",
    "    @property\n",
    "    def unwanted_columns(self) -> List[str]:\n",
    "        features = [\n",
    "            self.col_complaint_id,\n",
    "            self.col_sub_product,\n",
    "            self.col_complaint_what_happened,\n",
    "        ]\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **constant.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "@dataclass\n",
    "class EnvironmentVariable:\n",
    "    mongo_db_url = os.getenv(\"MONGO_DB_URL\")\n",
    "\n",
    "env_var = EnvironmentVariable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **config_entity.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_VALIDATION_DIR = \"data_validation\"\n",
    "DATA_VALIDATION_FILE_NAME = \"finance_complaint\"\n",
    "DATA_VALIDATION_ACCEPTED_DATA_DIR = \"accepted_data\"\n",
    "DATA_VALIDATION_REJECTED_DATA_DIR = \"rejected_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training pipeline config\n",
    "@dataclass\n",
    "class TrainingPipelineConfig:\n",
    "    pipeline_name: str = \"artifact\"\n",
    "    artifact_dir: str = os.path.join(pipeline_name, TIMESTAMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidationConfig:\n",
    "\n",
    "    def __init__(self, training_pipeline_config: TrainingPipelineConfig) -> None:\n",
    "        try:\n",
    "            data_validation_dir = os.path.join(training_pipeline_config.artifact_dir, \n",
    "                                               DATA_VALIDATION_DIR)\n",
    "            self.accepted_data_dir = os.path.join(data_validation_dir, DATA_VALIDATION_ACCEPTED_DATA_DIR)\n",
    "            self.rejected_data_dir = os.path.join(data_validation_dir, DATA_VALIDATION_REJECTED_DATA_DIR)\n",
    "            self.file_name = DATA_VALIDATION_FILE_NAME\n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **artifact_entity.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataValidationArtifact:\n",
    "    accepted_file_path: str\n",
    "    rejected_dir: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **data_validation.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoClient(host=['ac-zciqt62-shard-00-00.uhbxanv.mongodb.net:27017', 'ac-zciqt62-shard-00-02.uhbxanv.mongodb.net:27017', 'ac-zciqt62-shard-00-01.uhbxanv.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, retrywrites=True, w='majority', appname='mlcluster', authsource='admin', replicaset='atlas-10vcjq-shard-0', tls=True, server_api=<pymongo.server_api.ServerApi object at 0x1107ab380>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/04 21:08:09 WARN Utils: Your hostname, Rahuls-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.13 instead (on interface en0)\n",
      "25/05/04 21:08:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/04 21:08:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/04 21:08:11 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import os,sys\n",
    "from collections import namedtuple\n",
    "from typing import List, Dict\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from finance_complaint.config.spark_manager import spark_session\n",
    "from finance_complaint.entity.artifact_entity import DataIngestionArtifact\n",
    "# from finance_complaint.entity.config_entity import DataValidationConfig\n",
    "# from finance_complaint.entity.schema import FinanceDataSchema\n",
    "from finance_complaint.exception import FinanceException\n",
    "from finance_complaint.logger import logging as logger\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "# from finance_complaint.entity.artifact_entity import DataValidationArtifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR_MESSAGE = \"error_msg\"\n",
    "MissingReport = namedtuple(\"MissingReport\",\n",
    "                           [\"total_row\", \"missing_row\", \"missing_percentage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidation():\n",
    "    def __init__(self,\n",
    "                data_validation_config: DataValidationConfig,\n",
    "                data_ingestion_artifact: DataIngestionArtifact,\n",
    "                schema=FinanceDataSchema()\n",
    "                ):\n",
    "        try:\n",
    "            # super().__init__()\n",
    "            self.data_ingestion_artifact: DataIngestionArtifact = data_ingestion_artifact\n",
    "            self.data_validation_config = data_validation_config\n",
    "            self.schema = schema       \n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)\n",
    "        \n",
    "    def read_data(self) -> DataFrame:\n",
    "        \"\"\" \n",
    "        function to read a dataframe\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dataframe: DataFrame = spark_session.read.parquet(\n",
    "                self.data_ingestion_artifact.feature_store_file_path\n",
    "            ).limit(10000) # only reading 10 thiusands of records , when go for deployment will remove it\n",
    "            logger.info(f\"Data frame is created using file: {self.data_ingestion_artifact.feature_store_file_path}\")\n",
    "            logger.info(f\"Number of row: {dataframe.count()} and column: {len(dataframe.columns)}\")\n",
    "            return dataframe\n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_missing_report(dataframe: DataFrame, ) -> Dict[str, MissingReport]:\n",
    "        \"\"\" \n",
    "        this function is responsible for creating report of missing data for each columns\n",
    "        \"\"\"\n",
    "        try:\n",
    "            missing_report: Dict[str: MissingReport] = dict()\n",
    "            logger.info(f\"Prepraing missing reports for each column\")\n",
    "            number_of_row = dataframe.count()\n",
    "\n",
    "            for column in dataframe.columns:\n",
    "                missing_row = dataframe.filter(f\"{column} is null\").count()\n",
    "                missing_percentage = (missing_row * 100) / number_of_row\n",
    "                missing_report[column] = MissingReport(\n",
    "                    total_row=number_of_row,\n",
    "                    missing_row=missing_row,\n",
    "                    missing_percentage=missing_percentage\n",
    "                )\n",
    "            logger.info(f\"Missing report prepared: {missing_report}\")\n",
    "            return missing_report\n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)\n",
    "        \n",
    "    def get_unwanted_and_high_missing_value_columns(self, dataframe: DataFrame, threshold: float = 0.2) -> List[str]:\n",
    "        \"\"\" \n",
    "        this function removes columns with threshold missing values\n",
    "        \"\"\"\n",
    "        try:\n",
    "            missing_report: Dict[str, MissingReport] = self.get_missing_report(dataframe=dataframe)\n",
    "\n",
    "            unwanted_column: List[str] = self.schema.unwanted_columns\n",
    "            for column in missing_report:\n",
    "                if missing_report[column].missing_percentage > (threshold * 100):\n",
    "                    unwanted_column.append(column)\n",
    "                    logger.info(f\"Missing report {column}: [{missing_report[column]}]\")\n",
    "            unwanted_column = list(set(unwanted_column))\n",
    "\n",
    "            return unwanted_column\n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)\n",
    "        \n",
    "    def drop_unwanted_columns(self, dataframe: DataFrame) -> DataFrame:\n",
    "        \"\"\" \n",
    "        this function will drop the unwanted columns from the dataframe\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # get all unwanted columns to drop\n",
    "            unwanted_columns: List = self.get_unwanted_and_high_missing_value_columns(dataframe=dataframe,)\n",
    "            # log those columns\n",
    "            logger.info(f\"Droppping fetaures: {','.join(unwanted_columns)}\")\n",
    "            # after dropping columns get the dataframe\n",
    "            unwanted_dataframe: DataFrame = dataframe.select(unwanted_columns)\n",
    "            # adding one new column `ERROR_MESSAGE and for each rwo there is a value given in lit(\"message_\")` to move \n",
    "            # to rejected directory\n",
    "            unwanted_dataframe = unwanted_dataframe.withColumn(ERROR_MESSAGE, lit(\"Contains many missing values\"))\n",
    "\n",
    "            # preparing rejection directory\n",
    "            rejected_dir = os.path.join(self.data_validation_config.rejected_data_dir, \"missing_data\")\n",
    "            # creating directory in memory\n",
    "            os.makedirs(rejected_dir, exist_ok=True)\n",
    "            # prepare the file path\n",
    "            file_path = os.path.join(rejected_dir, self.data_validation_config.file_name)\n",
    "\n",
    "            # logging the file path\n",
    "            logger.info(f\"writing dropped column into file: [{file_path}]\")\n",
    "            # then write the content to file\n",
    "            unwanted_dataframe.write.mode('append').parquet(file_path)\n",
    "            # now from original data frame will drop the unwanted columns\n",
    "            dataframe: DataFrame = dataframe.drop(*unwanted_columns)\n",
    "            # logging the remaining columns of original dataframe\n",
    "            logger.info(f\"Remaining columns of dataframe: [{dataframe.columns}]\")\n",
    "            # return the dataframe which will have required columns\n",
    "            return dataframe\n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)\n",
    "        \n",
    "    def is_required_columns_exist(self, dataframe: DataFrame):\n",
    "        \"\"\" \n",
    "        this function checks if dataframe have required columns or not\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # check for required columns\n",
    "            columns = list(filter(lambda x: x in self.schema.required_columns, dataframe.columns))\n",
    "\n",
    "            # then check for length\n",
    "            if len(columns) != len(self.schema.required_columns):\n",
    "                raise Exception(f\"Required column missing\\n\\\n",
    "                                Expected columns: {self.schema.required_columns}\\n\\\n",
    "                                Found columns: {columns}\\\n",
    "                                    \")\n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)\n",
    "        \n",
    "    def initiate_data_validation(self) -> DataValidationArtifact:\n",
    "        \"\"\" \n",
    "        this function resonable to perform data validation\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"initiating data processing.\")\n",
    "            # 1st read the data\n",
    "            dataframe: DataFrame = self.read_data()\n",
    "\n",
    "            logger.info(f\"Dropping unwanted columns\")\n",
    "            # 2nd drop unwanted oclumns\n",
    "            dataframe: DataFrame = self.drop_unwanted_columns(dataframe=dataframe)\n",
    "\n",
    "            # validation to ensure that all required column available\n",
    "            self.is_required_columns_exist(dataframe=dataframe)\n",
    "            # logging log\n",
    "            logger.info(\"Saving preprocessed data.\")\n",
    "            # printing number of rows and columns\n",
    "            print(f\"Row: [{dataframe.count()}] Column: [{len(dataframe.columns)}]\")\n",
    "            # printing expected columns and present columns\n",
    "            print(f\"Expected Column: {self.schema.required_columns}\\nPresent Columns: {dataframe.columns}\")\n",
    "            # creating directory for accepted data\n",
    "            os.makedirs(self.data_validation_config.accepted_data_dir, exist_ok=True)\n",
    "            # preparing artifact path\n",
    "            accepted_file_path = os.path.join(self.data_validation_config.accepted_data_dir,\n",
    "                                              self.data_validation_config.file_name)\n",
    "            \n",
    "            # saving dataframe in parquet format to accepted folder\n",
    "            dataframe.write.parquet(accepted_file_path)\n",
    "            # preparing the artifact\n",
    "            artifact = DataValidationArtifact(\n",
    "                accepted_file_path=accepted_file_path,\n",
    "                rejected_dir=self.data_validation_config.rejected_data_dir\n",
    "            )\n",
    "            logger.info(f\"Data validation artifact: [{artifact}]\")\n",
    "            # returning artifact\n",
    "            return artifact\n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **training.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finance_complaint.entity.config_entity import DataIngestionConfig\n",
    "from finance_complaint.components.data_ingestion import DataIngestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingPipeline:\n",
    "    \"\"\" \n",
    "    this is a training pipeline comprising each components from flow chart\n",
    "    \"\"\"\n",
    "    def __init__(self, training_pipeline_config: TrainingPipelineConfig):\n",
    "        self.training_pipeline_config: TrainingPipelineConfig = training_pipeline_config\n",
    "\n",
    "    def start_data_ingestion(self) -> DataIngestionArtifact:\n",
    "        \"\"\"\n",
    "        responsible for starting data ingestion \n",
    "        \"\"\"\n",
    "        try:\n",
    "            data_ingestion_config = DataIngestionConfig(training_pipeline_config=self.training_pipeline_config)\n",
    "            data_ingestion = DataIngestion(data_ingestion_config=data_ingestion_config)\n",
    "            data_ingestion_artifact = data_ingestion.initiate_data_ingestion()\n",
    "            return data_ingestion_artifact\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error: {e}\")\n",
    "            raise FinanceException(e, sys)\n",
    "        \n",
    "    def start_data_validation(self, data_ingestion_artifact: DataIngestionArtifact) -> DataValidationArtifact:\n",
    "        \"\"\"\n",
    "        takes ingested data artifact and validate that data by generating validation artifact\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data_validation_config = DataValidationConfig(training_pipeline_config=self.training_pipeline_config)\n",
    "            data_validation = DataValidation(\n",
    "                data_validation_config=data_validation_config,\n",
    "                data_ingestion_artifact=data_ingestion_artifact\n",
    "            )\n",
    "            data_validation_artifact = data_validation.initiate_data_validation()\n",
    "            return data_validation_artifact\n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)\n",
    "        \n",
    "    def start(self):\n",
    "        try:\n",
    "            # initalizating data ingestion\n",
    "            data_ingestion_artifact = self.start_data_ingestion()\n",
    "            # initalizing data validation\n",
    "            data_validation_artifact = self.start_data_validation(data_ingestion_artifact=data_ingestion_artifact)\n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **train.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row: [10000] Column: [12]\n",
      "Expected Column: ['consumer_disputed', 'company_response', 'consumer_consent_provided', 'submitted_via', 'issue', 'date_sent_to_company', 'date_received']\n",
      "Present Columns: ['company', 'company_response', 'consumer_consent_provided', 'consumer_disputed', 'date_received', 'date_sent_to_company', 'issue', 'product', 'state', 'submitted_via', 'timely', 'zip_code']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    training_pipeline_config = TrainingPipelineConfig()\n",
    "    training_pipeline = TrainingPipeline(training_pipeline_config=training_pipeline_config)\n",
    "    training_pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
