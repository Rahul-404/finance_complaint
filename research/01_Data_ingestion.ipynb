{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01-Data-Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/rahulshelke/Documents/Data-Science/Data-Science-Projects/finance_complaint/research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/rahulshelke/Documents/Data-Science/Data-Science-Projects/finance_complaint'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Entity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **metadata_entity.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading environment variables\n",
      "Read Complete!\n"
     ]
    }
   ],
   "source": [
    "from finance_complaint.exception import FinanceException\n",
    "from finance_complaint.logger import logging as logger\n",
    "from finance_complaint.utils import write_yaml_file, read_yaml_file\n",
    "from collections import namedtuple\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from finance_complaint.entity.config_entity import DataIngestionConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataIngestionMetadataInfo = namedtuple(\"DataIngestionMetadataInfo\",\n",
    "                                       [\"from_date\", \"to_date\", \"data_file_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestionMetadata:\n",
    "    def __init__(self, metadata_file_path,):\n",
    "        self.metadata_file_path = metadata_file_path\n",
    "\n",
    "    @property\n",
    "    def is_metadata_file_present(self) -> bool:\n",
    "        ''' \n",
    "        checks if metadata file is present or not\n",
    "        returns: bool\n",
    "        '''\n",
    "        return os.path.exists(self.metadata_file_path)\n",
    "    \n",
    "    def write_metadata_info(self, from_date: str, to_date: str, data_file_path: str):\n",
    "        ''' \n",
    "        this will get the from_date, to_date and path to store yaml file\n",
    "        then it will store data in yaml file\n",
    "        \n",
    "        Params:\n",
    "            from_date: str\n",
    "            to_date: str\n",
    "            date_file_path: str\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        try:\n",
    "            metadata_info = DataIngestionMetadataInfo(\n",
    "                from_date=from_date,\n",
    "                to_date=to_date,\n",
    "                data_file_path=data_file_path\n",
    "            )\n",
    "            write_yaml_file(file_path=self.metadata_file_path, data=metadata_info._asdict())\n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)\n",
    "        \n",
    "    def get_metadata_info(self) ->  DataIngestionMetadataInfo:\n",
    "        '''\n",
    "        this will read existing metadata file and return the info\n",
    "        '''\n",
    "        try:\n",
    "            if not self.is_metadata_file_present:\n",
    "                raise Exception(\"no metadata file available\")\n",
    "            metadata = read_yaml_file(self.metadata_file_path)\n",
    "            metadata_info = DataIngestionMetadataInfo(**(metadata))\n",
    "            logger.info(metadata)\n",
    "            return metadata_info\n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **constant.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "@dataclass\n",
    "class EnvironmentVariable:\n",
    "    mongo_db_url = os.getenv(\"MONGO_DB_URL\")\n",
    "\n",
    "env_var = EnvironmentVariable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **config entity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from finance_complaint.components import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data ingestion constants\n",
    "DATA_INGESTION_DIR = \"data_ingestion\"\n",
    "DATA_INGESTION_DOWNLOAD_DATA_DIR = \"download_files\"\n",
    "DATA_INGESTION_FILE_NAME = \"finance_complaint\"\n",
    "DATA_INGESTION_FEATURE_STORE_DIR = \"feature_store\"\n",
    "DATA_INGESTION_FAILED_DIR = \"failed_download_files\"\n",
    "DATA_INGESTION_METADATA_FILE_NAME = \"meta_info.yaml\"\n",
    "DATA_INGESTION_MIN_START_DATE = \"2013-01-01\"\n",
    "DATA_INGESTION_DATA_SOURCE_URL = f\"https://www.consumerfinance.gov/data-research/consumer-complaints/search/api/v1/\"\\\n",
    "                                f\"?date_received_max=<todate>&date_received_min=<fromdate>\"\\\n",
    "                                f\"&field=all&format=json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.consumerfinance.gov/data-research/consumer-complaints/search/api/v1/?date_received_max=<todate>&date_received_min=<fromdate>&field=all&format=json'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_INGESTION_DATA_SOURCE_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training pipeline config\n",
    "@dataclass\n",
    "class TrainingPipelineConfig:\n",
    "    pipeline_name: str = \"artifact\"\n",
    "    artifact_dir: str = os.path.join(pipeline_name, TIMESTAMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data ingestion config class\n",
    "class DataIngestionConfig:\n",
    "    def __init__(self, training_pipeline_config: TrainingPipelineConfig,\n",
    "                 from_date=DATA_INGESTION_MIN_START_DATE,\n",
    "                 to_date=None):\n",
    "        try:\n",
    "            # get the from data 1st\n",
    "            self.from_date=from_date\n",
    "            # convert provided date to format yy-mm-dd\n",
    "            min_start_date = datetime.strptime(DATA_INGESTION_MIN_START_DATE, \"%Y-%m-%d\")\n",
    "            # also converting from date to format yy-mm-dd\n",
    "            from_date_obj = datetime.strptime(from_date, \"%Y-%m-%d\")\n",
    "\n",
    "            # check if min start date is greater: it supposed to be to get new data\n",
    "            if from_date_obj < min_start_date:\n",
    "                self.from_date = DATA_INGESTION_MIN_START_DATE\n",
    "\n",
    "            # now to date is last date till which data will be downloaded, i.e current date\n",
    "            if to_date is None:\n",
    "                self.to_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "            else:\n",
    "                self.to_date = to_date\n",
    "\n",
    "            # this is path to `artifact` -> `data_ingestion` directory\n",
    "            data_ingestion_master_dir = os.path.join(os.path.dirname(training_pipeline_config.artifact_dir), DATA_INGESTION_DIR)\n",
    "            # nested directory in side `artifact` -> `data_ingestion` -> `1999-01-01``\n",
    "            self.data_ingestion_dir = os.path.join(data_ingestion_master_dir, TIMESTAMP)\n",
    "            # getting meta data file path\n",
    "            self.metadata_file_path = os.path.join(data_ingestion_master_dir, DATA_INGESTION_METADATA_FILE_NAME)\n",
    "            # using above path will ingest meta data info\n",
    "            data_ingestion_metadata = DataIngestionMetadata(metadata_file_path=self.metadata_file_path)\n",
    "\n",
    "            # check if meta data file exists\n",
    "            if data_ingestion_metadata.is_metadata_file_present:\n",
    "                # if exists fetch the data \n",
    "                metadata_info = data_ingestion_metadata.get_metadata_info()\n",
    "                # now from exisiting metadata info make last to_date to current from_date\n",
    "                self.from_date = metadata_info.to_date\n",
    "\n",
    "            # location where we are going to download the data\n",
    "            self.download_dir = os.path.join(self.data_ingestion_dir, DATA_INGESTION_DOWNLOAD_DATA_DIR)\n",
    "\n",
    "            # location to store stor filed files downloads\n",
    "            self.failed_dir = os.path.join(self.data_ingestion_dir, DATA_INGESTION_FAILED_DIR)\n",
    "\n",
    "            # name to the file (json file) which will store in above directory\n",
    "            self.file_name = DATA_INGESTION_FILE_NAME\n",
    "\n",
    "            # now will convert all json files to parquet and store to feature store\n",
    "            self.feature_store_dir = os.path.join(data_ingestion_master_dir, DATA_INGESTION_FEATURE_STORE_DIR)\n",
    "\n",
    "            # url from where will download data\n",
    "            self.datasource_url = DATA_INGESTION_DATA_SOURCE_URL\n",
    "\n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline_config = TrainingPipelineConfig()\n",
    "data_ingestion_config = DataIngestionConfig(training_pipeline_config=training_pipeline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingPipelineConfig(pipeline_name='artifact', artifact_dir='artifact/20250423_151754')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_pipeline_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DataIngestionConfig at 0x108e81670>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ingestion_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **artifact config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now here will define the output structure, as per output we only want following structure\n",
    "\n",
    "@dataclass\n",
    "class DataIngestionArtifact:\n",
    "    ''' \n",
    "    config to manage any artifacts from data ingestion stage\n",
    "    '''\n",
    "    feature_store_file_path: str # path to store parquet file\n",
    "    metadata_file_path: str      # path to check meta data info\n",
    "    download_dir: str            # source api url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **data ingestion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoClient(host=['ac-zciqt62-shard-00-02.uhbxanv.mongodb.net:27017', 'ac-zciqt62-shard-00-00.uhbxanv.mongodb.net:27017', 'ac-zciqt62-shard-00-01.uhbxanv.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, retrywrites=True, w='majority', appname='mlcluster', authsource='admin', replicaset='atlas-10vcjq-shard-0', tls=True, server_api=<pymongo.server_api.ServerApi object at 0x12fd4f380>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/23 15:17:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "# from finance_complaint.entity.artifact_entity import DataIngestionArtifact\n",
    "# from finance_complaint.entity.metadata_entity import DataIngestionMetadata\n",
    "# from finance_complaint.entity.config_entity import DataIngestionConfig\n",
    "from finance_complaint.exception import FinanceException\n",
    "from finance_complaint.logger import logging as logger\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import os, sys\n",
    "\n",
    "from finance_complaint.config.spark_manager import spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DownloadUrl = namedtuple(\n",
    "    \"DownloadUrl\", [\"url\", \"file_path\", \"n_retry\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestion:\n",
    "    # used to download data in chunks.\n",
    "    def __init__(self, data_ingestion_config: DataIngestionConfig, n_retry: int = 5,):\n",
    "        \"\"\" \n",
    "        data_ingestion_config: Data Ingestion config\n",
    "        n_retry: Number of retry filed should be tried to download in case of failuer encountered\n",
    "        n_month_interval: n month data will be downloaded \n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"{'>>'*20} Strting data ingestion.{'<<'*20}\")\n",
    "            self.data_ingestion_config = data_ingestion_config\n",
    "            self.failed_download_urls: List[DownloadUrl] = []\n",
    "            self.n_retry = n_retry\n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)\n",
    "        \n",
    "    def get_required_interval(self):\n",
    "        '''\n",
    "        if date gap is too huge, i.e 2011 to 2022 size can be huge\n",
    "\n",
    "        so instead of downloading entire data at once\n",
    "        we can download yearly data like\n",
    "        2011 - 2012\n",
    "        2012 - 2013\n",
    "        . \n",
    "        . \n",
    "        2021 - 2022\n",
    "        following is the function\n",
    "        '''\n",
    "        # reading dates from config start and end date in yy-mm-dd format \n",
    "        start_date = datetime.strptime(self.data_ingestion_config.from_date, \"%Y-%m-%d\")\n",
    "        end_date = datetime.strptime(self.data_ingestion_config.to_date, \"%Y-%m-%d\")\n",
    "        # checing number of days between end and start date\n",
    "        n_diff_days = (end_date - start_date).days\n",
    "        freq = None\n",
    "        # if diff is more than 365\n",
    "        if n_diff_days > 365:\n",
    "            freq = \"YE\"\n",
    "        # else if diff is more than 30 \n",
    "        elif n_diff_days > 30:\n",
    "            freq = \"ME\"\n",
    "        # else if diff is more than 7\n",
    "        elif n_diff_days > 7:\n",
    "            freq = \"W\"\n",
    "        logger.debug(f\"{n_diff_days} hance freq: {freq}\")\n",
    "\n",
    "        # still if frequency is None mean less than 7\n",
    "        if freq is None:\n",
    "            intervals = pd.date_range(\n",
    "                start = self.data_ingestion_config.from_date,\n",
    "                end = self.data_ingestion_config.to_date,\n",
    "                periods=2\n",
    "            ).astype('str').tolist()\n",
    "        else:\n",
    "            intervals = pd.date_range(\n",
    "                start=self.data_ingestion_config.from_date,\n",
    "                end=self.data_ingestion_config.to_date,\n",
    "                freq=freq\n",
    "            ).astype('str').tolist()\n",
    "        logger.debug(f\"Prepared Interval: {intervals}\")\n",
    "\n",
    "        # check if to_date is not in intervals ? -> then add it to intervals list\n",
    "        if self.data_ingestion_config.to_date not in intervals:\n",
    "            intervals.append(self.data_ingestion_config.to_date)\n",
    "\n",
    "        return intervals\n",
    "    \n",
    "    def download_files(self, n_day_interval_url: int = None):\n",
    "        \"\"\" \n",
    "        n_month_interval_url: if not provided then information default value will be set\n",
    "        =================================================================================\n",
    "        returns: List of DownloadUrl = namestuple(\"DownloadUrl\", [\"url\", \"file_path\", \"n_retry\"])\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # here will call function to get interval list\n",
    "            required_interval = self.get_required_interval()\n",
    "            logger.info(\"Started downloading files\")\n",
    "\n",
    "            # iterate over the interval\n",
    "            for index in range(1, len(required_interval)):\n",
    "                # fetching from_date and to_date\n",
    "                from_date, to_date = required_interval[index - 1], required_interval[index]\n",
    "                logger.debug(f\"Generating data download url between {from_date} and {to_date}\")\n",
    "\n",
    "                # getting data source url\n",
    "                datasource_url: str = self.data_ingestion_config.datasource_url\n",
    "                # replacing the exisiting values with new from_date and to_date\n",
    "                url = datasource_url.replace(\"<todate>\", to_date).replace(\"<fromdate>\", from_date)\n",
    "\n",
    "                logger.debug(f\"Url: {url}\")\n",
    "\n",
    "                # creating json file name\n",
    "                file_name = f\"{self.data_ingestion_config.file_name}_{from_date}_{to_date}.json\"\n",
    "                # getting complete path to json file\n",
    "                file_path = os.path.join(self.data_ingestion_config.download_dir, file_name)\n",
    "\n",
    "                # download url\n",
    "                download_url = DownloadUrl(url=url, file_path=file_path, n_retry=self.n_retry)\n",
    "\n",
    "                # this function downloads data from passed url\n",
    "                self.download_data(download_url=download_url)\n",
    "\n",
    "            logger.info(f\"File download completed\")\n",
    "        except Exception as e:  \n",
    "            raise FinanceException(e, sys)\n",
    "        \n",
    "    def download_data(self, download_url: DownloadUrl):\n",
    "        \"\"\" \n",
    "        this function responsible to download data and store it to data ingestion directory\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Starting download operation: {download_url}\")\n",
    "            download_dir = os.path.dirname(download_url.file_path)\n",
    "\n",
    "            # creating download directory\n",
    "            os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "            # downloading data: by requesting to the url\n",
    "            data = requests.get(download_url.url, params={'User-agent': f'your bot {uuid.uuid4()}'})\n",
    "\n",
    "            try:\n",
    "                logger.info(f\"Started writing downloaded data into json file: {download_url.file_path}\")\n",
    "                # saving downloaded data into hard disk\n",
    "                with open(download_url.file_path, \"w\") as file_obj:\n",
    "                    finance_complaint_data = list(map(lambda x: x[\"_source\"], filter(lambda x: \"_source\" in x.keys(), json.loads(data.content))))\n",
    "                    json.dump(finance_complaint_data, file_obj)\n",
    "                logger.info(f\"Download data has been written into file: {download_url.file_path}\")\n",
    "            except Exception as e:\n",
    "                logger.info(\"Failed to download hance retry again.\")\n",
    "                # removing file failed file exists\n",
    "                if os.path.exists(download_url.file_path):\n",
    "                    os.remove(download_url.file_path)\n",
    "                self.retry_download_data(data, download_url=download_url)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.info(e)\n",
    "            raise FinanceException(e, sys)\n",
    "        \n",
    "    def retry_download_data(self, data, download_url: DownloadUrl):\n",
    "        \"\"\" \n",
    "        This function helps to avoid failure as it help to download failed file again\n",
    "\n",
    "        data: failed response\n",
    "        download_url: DownloadUrl\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # if retry still possible try else return the response\n",
    "            if download_url.n_retry == 0:\n",
    "                self.failed_download_urls.append(download_url.url)\n",
    "                logger.info(f\"Unable to download file: {download_url.url}\")\n",
    "                return\n",
    "            \n",
    "            # if in case it is not zero, to handle throatling requestion and can be solve if we wait for some second.\n",
    "            content = data.content.decode(\"utf-8\")\n",
    "            wait_second = re.findall(r'\\d+', content)\n",
    "\n",
    "            if len(wait_second) > 0:\n",
    "                time.sleep(int(wait_second[0]) + 2)\n",
    "\n",
    "            # Writing response to understand why request was failed\n",
    "            failed_file_path = os.path.join(self.data_ingestion_config.failed_dir,\n",
    "                                            os.path.basename(download_url.file_path))\n",
    "            os.makedirs(self.data_ingestion_config.failed_dir, exist_ok=True)\n",
    "            with open(failed_file_path, \"wb\") as file_obj:\n",
    "                file_obj.write(data.content)\n",
    "\n",
    "            # calling download function again to retry\n",
    "            download_url = DownloadUrl(download_url.url, file_path=download_url.file_path,\n",
    "                                       n_retry=download_url.n_retry - 1)\n",
    "            self.download_data(download_url=download_url)\n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)\n",
    "        \n",
    "    def convert_files_to_parquet(self,) -> str:\n",
    "        \"\"\" \n",
    "        downloaded files will be converted and merged into single parquet file\n",
    "        json_data_dir: downloaded json file directory\n",
    "        data_dir: convrted and combine file will be generated in data_dir\n",
    "        output_file_name: output file name\n",
    "        ========================================================================\n",
    "        returns output_file_path\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # json file download directory\n",
    "            json_data_dir = self.data_ingestion_config.download_dir\n",
    "            # feature store directoy \n",
    "            data_dir = self.data_ingestion_config.feature_store_dir\n",
    "            # aftre merging file will get a paruqet file, so its parquet file name\n",
    "            output_file_name = self.data_ingestion_config.file_name\n",
    "            #  creating features store folder\n",
    "            os.makedirs(data_dir, exist_ok=True)\n",
    "            file_path = os.path.join(data_dir, f\"{output_file_name}\")\n",
    "            logger.info(f\"Parquet file will be created at: {file_path}\")\n",
    "            # check 1st do we have json directory or not?\n",
    "            if not os.path.exists(json_data_dir):\n",
    "                return file_path\n",
    "            \n",
    "            # after listing all the files will iterate over it\n",
    "            for file_name in os.listdir(json_data_dir):\n",
    "                json_data_path = os.path.join(json_data_dir, file_name)\n",
    "                logger.debug(f\"Converting {json_data_path} into parquet format at {file_path}\")\n",
    "\n",
    "                df = spark_session.read.json(json_data_path)\n",
    "                if df.count() > 0:\n",
    "                    df.write.mode('append').parquet(file_path)\n",
    "            return file_path\n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)\n",
    "    \n",
    "    def write_metadata(self, file_path: str) -> None:\n",
    "        \"\"\" \n",
    "        This function help us to update metadata information\n",
    "        so that we can avoid redundant download and merging.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Write metadata into into metadata file\")\n",
    "            metadata_info = DataIngestionMetadata(\n",
    "                metadata_file_path=self.data_ingestion_config.metadata_file_path,\n",
    "            )\n",
    "\n",
    "            metadata_info.write_metadata_info(\n",
    "                from_date=self.data_ingestion_config.from_date,\n",
    "                to_date=self.data_ingestion_config.to_date,\n",
    "                data_file_path=file_path\n",
    "            )\n",
    "            logger.info(f\"Metadata had been written.\")\n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)\n",
    "        \n",
    "    def initiate_data_ingestion(self) -> DataIngestionArtifact:\n",
    "        \"\"\" \n",
    "        this function responsible to initiate data ingestion\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Started downloading json file\")\n",
    "\n",
    "            # check if we downloading new data or not\n",
    "            if self.data_ingestion_config.from_date != self.data_ingestion_config.to_date:\n",
    "                self.download_files()\n",
    "\n",
    "            # if json data exists start converting to parquet and update the metadata info\n",
    "            if os.path.exists(self.data_ingestion_config.download_dir):\n",
    "                logger.info(f\"Converting and combining downloaded json into parquet file.\")\n",
    "                file_path = self.convert_files_to_parquet()\n",
    "                self.write_metadata(file_path=file_path)\n",
    "\n",
    "            # path to new parquet file\n",
    "            feature_store_file_path = os.path.join(self.data_ingestion_config.feature_store_dir,\n",
    "                                                   self.data_ingestion_config.file_name)\n",
    "            \n",
    "            # store it to artifact\n",
    "            artifact = DataIngestionArtifact(\n",
    "                feature_store_file_path=feature_store_file_path,\n",
    "                download_dir=self.data_ingestion_config.download_dir,\n",
    "                metadata_file_path=self.data_ingestion_config.metadata_file_path,\n",
    "            )\n",
    "            logger.info(f\"Data ingestion artifact: {artifact}\")\n",
    "\n",
    "            return artifact\n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing : Data Ingestion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DataIngestion at 0x108d96ff0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ingestion = DataIngestion(data_ingestion_config, 5)\n",
    "\n",
    "data_ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2013-12-31',\n",
       " '2014-12-31',\n",
       " '2015-12-31',\n",
       " '2016-12-31',\n",
       " '2017-12-31',\n",
       " '2018-12-31',\n",
       " '2019-12-31',\n",
       " '2020-12-31',\n",
       " '2021-12-31',\n",
       " '2022-12-31',\n",
       " '2023-12-31',\n",
       " '2024-12-31',\n",
       " '2025-04-23']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ingestion.get_required_interval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finance_complaint.exception import FinanceException\n",
    "from finance_complaint.logger import logging as logger\n",
    "# from finance_complaint.entity.config_entity import DataIngestionConfig,\n",
    "# from finance_complaint.entity.artifact_entity import DataIngestionArtifact,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class TrainingPipeline:\n",
    "\n",
    "    def __init__(self, training_pipeline_config: TrainingPipelineConfig):\n",
    "        self.training_pipeline_config: TrainingPipelineConfig = training_pipeline_config\n",
    "\n",
    "    def start_date_ingestion(self) -> DataIngestionArtifact:\n",
    "        try:\n",
    "            data_ingestion_config = DataIngestionConfig(training_pipeline_config=self.training_pipeline_config)\n",
    "            data_ingestion = DataIngestion(data_ingestion_config=data_ingestion_config)\n",
    "            data_ingestion_artifact = data_ingestion.initiate_data_ingestion()\n",
    "            return data_ingestion_artifact\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)\n",
    "        \n",
    "    def start(self):\n",
    "        try:\n",
    "            data_ingestion_artifact = self.start_date_ingestion()\n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **train.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    training_pipeline_config = TrainingPipelineConfig()\n",
    "    training_pipeline = TrainingPipeline(training_pipeline_config=training_pipeline_config)\n",
    "    training_pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- runs for around 150 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
