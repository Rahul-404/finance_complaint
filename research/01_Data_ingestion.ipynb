{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01-Data-Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/rahulshelke/Documents/Data-Science/Data-Science-Projects/finance_complaint/research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/rahulshelke/Documents/Data-Science/Data-Science-Projects/finance_complaint'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Entity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **metadata_entity.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading environment variables\n",
      "Read Complete!\n"
     ]
    }
   ],
   "source": [
    "from finance_complaint.exception import FinanceException\n",
    "from finance_complaint.logger import logging as logger\n",
    "from finance_complaint.utils import write_yaml_file, read_yaml_file\n",
    "from collections import namedtuple\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from finance_complaint.entity.config_entity import DataIngestionConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataIngestionMetadataInfo = namedtuple(\"DataIngestionMetadataInfo\",\n",
    "                                       [\"from_date\", \"to_date\", \"data_file_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestionMetadata:\n",
    "    def __init__(self, metadata_file_path,):\n",
    "        self.metadata_file_path = metadata_file_path\n",
    "\n",
    "    @property\n",
    "    def is_metadata_file_present(self) -> bool:\n",
    "        ''' \n",
    "        checks if metadata file is present or not\n",
    "        returns: bool\n",
    "        '''\n",
    "        return os.path.exists(self.metadata_file_path)\n",
    "    \n",
    "    def write_metadata_info(self, from_date: str, to_date: str, data_file_path: str):\n",
    "        ''' \n",
    "        this will get the from_date, to_date and path to store yaml file\n",
    "        then it will store data in yaml file\n",
    "        \n",
    "        Params:\n",
    "            from_date: str\n",
    "            to_date: str\n",
    "            date_file_path: str\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        try:\n",
    "            metadata_info = DataIngestionMetadataInfo(\n",
    "                from_date=from_date,\n",
    "                to_date=to_date,\n",
    "                data_file_path=data_file_path\n",
    "            )\n",
    "            write_yaml_file(file_path=self.metadata_file_path, data=metadata_info._asdict())\n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)\n",
    "        \n",
    "    def get_metadata_info(self) ->  DataIngestionMetadataInfo:\n",
    "        '''\n",
    "        this will read existing metadata file and return the info\n",
    "        '''\n",
    "        try:\n",
    "            if not self.is_metadata_file_present:\n",
    "                raise Exception(\"no metadata file available\")\n",
    "            metadata = read_yaml_file(self.metadata_file_path)\n",
    "            metadata_info = DataIngestionMetadataInfo(**(metadata))\n",
    "            logger.info(metadata)\n",
    "            return metadata_info\n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **constant.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "@dataclass\n",
    "class EnvironmentVariable:\n",
    "    mongo_db_url = os.getenv(\"MONGO_DB_URL\")\n",
    "\n",
    "env_var = EnvironmentVariable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **config entity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from finance_complaint.components import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data ingestion constants\n",
    "DATA_INGESTION_DIR = \"data_ingestion\"\n",
    "DATA_INGESTION_DOWNLOAD_DATA_DIR = \"download_files\"\n",
    "DATA_INGESTION_FILE_NAME = \"finance_complaint\"\n",
    "DATA_INGESTION_FEATURE_STORE_DIR = \"feature_store\"\n",
    "DATA_INGESTION_FAILED_DIR = \"failed_download_files\"\n",
    "DATA_INGESTION_METADATA_FILE_NAME = \"meta_info.yaml\"\n",
    "DATA_INGESTION_MIN_START_DATE = \"2016-01-01\"\n",
    "DATA_INGESTION_DATA_SOURCE_URL = f\"https://www.consumerfinance.gov/data-research/consumer-complaints/search/api/v1/\"\\\n",
    "                                f\"?date_received_max=<todate>&date_received_min=<fromdate>\"\\\n",
    "                                f\"&field=all&format=json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.consumerfinance.gov/data-research/consumer-complaints/search/api/v1/?date_received_max=<todate>&date_received_min=<fromdate>&field=all&format=json'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_INGESTION_DATA_SOURCE_URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training pipeline config\n",
    "@dataclass\n",
    "class TrainingPipelineConfig:\n",
    "    pipeline_name: str = \"artifact\"\n",
    "    artifact_dir: str = os.path.join(pipeline_name, TIMESTAMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data ingestion config class\n",
    "class DataIngestionConfig:\n",
    "    def __init__(self, training_pipeline_config: TrainingPipelineConfig,\n",
    "                 from_date=DATA_INGESTION_MIN_START_DATE,\n",
    "                 to_date=None):\n",
    "        try:\n",
    "            # get the from data 1st\n",
    "            self.from_date=from_date\n",
    "            # convert provided date to format yy-mm-dd\n",
    "            min_start_date = datetime.strptime(DATA_INGESTION_MIN_START_DATE, \"%Y-%m-%d\")\n",
    "            # also converting from date to format yy-mm-dd\n",
    "            from_date_obj = datetime.strptime(from_date, \"%Y-%m-%d\")\n",
    "\n",
    "            # check if min start date is greater: it supposed to be to get new data\n",
    "            if from_date_obj < min_start_date:\n",
    "                self.from_date = DATA_INGESTION_MIN_START_DATE\n",
    "\n",
    "            # now to date is last date till which data will be downloaded, i.e current date\n",
    "            if to_date is None:\n",
    "                self.to_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "            else:\n",
    "                self.to_date = to_date\n",
    "\n",
    "            # this is path to `artifact` -> `data_ingestion` directory\n",
    "            data_ingestion_master_dir = os.path.join(os.path.dirname(training_pipeline_config.artifact_dir), DATA_INGESTION_DIR)\n",
    "            # nested directory in side `artifact` -> `data_ingestion` -> `1999-01-01``\n",
    "            self.data_ingestion_dir = os.path.join(data_ingestion_master_dir, TIMESTAMP)\n",
    "            # getting meta data file path\n",
    "            self.metadata_file_path = os.path.join(data_ingestion_master_dir, DATA_INGESTION_METADATA_FILE_NAME)\n",
    "            # using above path will ingest meta data info\n",
    "            data_ingestion_metadata = DataIngestionMetadata(metadata_file_path=self.metadata_file_path)\n",
    "\n",
    "            # check if meta data file exists\n",
    "            if data_ingestion_metadata.is_metadata_file_present:\n",
    "                # if exists fetch the data \n",
    "                metadata_info = data_ingestion_metadata.get_metadata_info()\n",
    "                # now from exisiting metadata info make last to_date to current from_date\n",
    "                self.from_date = metadata_info.to_date\n",
    "\n",
    "            # location where we are going to download the data\n",
    "            self.download_dir = os.path.join(self.data_ingestion_dir, DATA_INGESTION_DOWNLOAD_DATA_DIR)\n",
    "\n",
    "            # location to store stor filed files downloads\n",
    "            self.failed_dir = os.path.join(self.data_ingestion_dir, DATA_INGESTION_FAILED_DIR)\n",
    "\n",
    "            # name to the file (json file) which will store in above directory\n",
    "            self.file_name = DATA_INGESTION_FILE_NAME\n",
    "\n",
    "            # now will convert all json files to parquet and store to feature store\n",
    "            self.feature_store_dir = os.path.join(data_ingestion_master_dir, DATA_INGESTION_FEATURE_STORE_DIR)\n",
    "\n",
    "            # url from where will download data\n",
    "            self.datasource_url = DATA_INGESTION_DATA_SOURCE_URL\n",
    "\n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline_config = TrainingPipelineConfig()\n",
    "data_ingestion_config = DataIngestionConfig(training_pipeline_config=training_pipeline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingPipelineConfig(pipeline_name='artifact', artifact_dir='artifact/20250424_092545')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_pipeline_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DataIngestionConfig at 0x1100892b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ingestion_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **artifact config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now here will define the output structure, as per output we only want following structure\n",
    "\n",
    "@dataclass\n",
    "class DataIngestionArtifact:\n",
    "    ''' \n",
    "    config to manage any artifacts from data ingestion stage\n",
    "    '''\n",
    "    feature_store_file_path: str # path to store parquet file\n",
    "    metadata_file_path: str      # path to check meta data info\n",
    "    download_dir: str            # source api url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **data ingestion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoClient(host=['ac-zciqt62-shard-00-00.uhbxanv.mongodb.net:27017', 'ac-zciqt62-shard-00-02.uhbxanv.mongodb.net:27017', 'ac-zciqt62-shard-00-01.uhbxanv.mongodb.net:27017'], document_class=dict, tz_aware=False, connect=True, retrywrites=True, w='majority', appname='mlcluster', authsource='admin', replicaset='atlas-10vcjq-shard-0', tls=True, server_api=<pymongo.server_api.ServerApi object at 0x154c19760>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/24 09:26:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "# from finance_complaint.entity.artifact_entity import DataIngestionArtifact\n",
    "# from finance_complaint.entity.metadata_entity import DataIngestionMetadata\n",
    "# from finance_complaint.entity.config_entity import DataIngestionConfig\n",
    "from finance_complaint.exception import FinanceException\n",
    "from finance_complaint.logger import logging as logger\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import os, sys\n",
    "\n",
    "from finance_complaint.config.spark_manager import spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DownloadUrl = namedtuple(\n",
    "    \"DownloadUrl\", [\"url\", \"file_path\", \"n_retry\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestion:\n",
    "    # used to download data in chunks.\n",
    "    def __init__(self, data_ingestion_config: DataIngestionConfig, n_retry: int = 5,):\n",
    "        \"\"\" \n",
    "        data_ingestion_config: Data Ingestion config\n",
    "        n_retry: Number of retry filed should be tried to download in case of failuer encountered\n",
    "        n_month_interval: n month data will be downloaded \n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"{'>>'*20} Strting data ingestion.{'<<'*20}\")\n",
    "            self.data_ingestion_config = data_ingestion_config\n",
    "            self.failed_download_urls: List[DownloadUrl] = []\n",
    "            self.n_retry = n_retry\n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)\n",
    "        \n",
    "    def get_required_interval(self):\n",
    "        '''\n",
    "        if date gap is too huge, i.e 2011 to 2022 size can be huge\n",
    "\n",
    "        so instead of downloading entire data at once\n",
    "        we can download yearly data like\n",
    "        2011 - 2012\n",
    "        2012 - 2013\n",
    "        . \n",
    "        . \n",
    "        2021 - 2022\n",
    "        following is the function\n",
    "        '''\n",
    "        # reading dates from config start and end date in yy-mm-dd format \n",
    "        start_date = datetime.strptime(self.data_ingestion_config.from_date, \"%Y-%m-%d\")\n",
    "        end_date = datetime.strptime(self.data_ingestion_config.to_date, \"%Y-%m-%d\")\n",
    "        # checing number of days between end and start date\n",
    "        n_diff_days = (end_date - start_date).days\n",
    "        freq = None\n",
    "        # if diff is more than 365\n",
    "        if n_diff_days > 365:\n",
    "            freq = \"YE\"\n",
    "        # else if diff is more than 30 \n",
    "        elif n_diff_days > 30:\n",
    "            freq = \"ME\"\n",
    "        # else if diff is more than 7\n",
    "        elif n_diff_days > 7:\n",
    "            freq = \"W\"\n",
    "        logger.debug(f\"{n_diff_days} hance freq: {freq}\")\n",
    "\n",
    "        # still if frequency is None mean less than 7\n",
    "        if freq is None:\n",
    "            intervals = pd.date_range(\n",
    "                start = self.data_ingestion_config.from_date,\n",
    "                end = self.data_ingestion_config.to_date,\n",
    "                periods=2\n",
    "            ).astype('str').tolist()\n",
    "        else:\n",
    "            intervals = pd.date_range(\n",
    "                start=self.data_ingestion_config.from_date,\n",
    "                end=self.data_ingestion_config.to_date,\n",
    "                freq=freq\n",
    "            ).astype('str').tolist()\n",
    "        logger.debug(f\"Prepared Interval: {intervals}\")\n",
    "\n",
    "        # check if to_date is not in intervals ? -> then add it to intervals list\n",
    "        if self.data_ingestion_config.to_date not in intervals:\n",
    "            intervals.append(self.data_ingestion_config.to_date)\n",
    "\n",
    "        return intervals\n",
    "    \n",
    "    def download_files(self, n_day_interval_url: int = None):\n",
    "        \"\"\" \n",
    "        n_month_interval_url: if not provided then information default value will be set\n",
    "        =================================================================================\n",
    "        returns: List of DownloadUrl = namestuple(\"DownloadUrl\", [\"url\", \"file_path\", \"n_retry\"])\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # here will call function to get interval list\n",
    "            required_interval = self.get_required_interval()\n",
    "            logger.info(\"Started downloading files\")\n",
    "\n",
    "            # iterate over the interval\n",
    "            for index in range(1, len(required_interval)):\n",
    "                # fetching from_date and to_date\n",
    "                from_date, to_date = required_interval[index - 1], required_interval[index]\n",
    "                logger.debug(f\"Generating data download url between {from_date} and {to_date}\")\n",
    "\n",
    "                # getting data source url\n",
    "                datasource_url: str = self.data_ingestion_config.datasource_url\n",
    "                # replacing the exisiting values with new from_date and to_date\n",
    "                url = datasource_url.replace(\"<todate>\", to_date).replace(\"<fromdate>\", from_date)\n",
    "\n",
    "                logger.debug(f\"Url: {url}\")\n",
    "\n",
    "                # creating json file name\n",
    "                file_name = f\"{self.data_ingestion_config.file_name}_{from_date}_{to_date}.json\"\n",
    "                # getting complete path to json file\n",
    "                file_path = os.path.join(self.data_ingestion_config.download_dir, file_name)\n",
    "\n",
    "                # download url\n",
    "                download_url = DownloadUrl(url=url, file_path=file_path, n_retry=self.n_retry)\n",
    "\n",
    "                # this function downloads data from passed url\n",
    "                self.download_data(download_url=download_url)\n",
    "\n",
    "            logger.info(f\"File download completed\")\n",
    "        except Exception as e:  \n",
    "            raise FinanceException(e, sys)\n",
    "        \n",
    "    def download_data(self, download_url: DownloadUrl):\n",
    "        \"\"\" \n",
    "        this function responsible to download data and store it to data ingestion directory\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Starting download operation: {download_url}\")\n",
    "            download_dir = os.path.dirname(download_url.file_path)\n",
    "\n",
    "            # creating download directory\n",
    "            os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "            # downloading data: by requesting to the url\n",
    "            data = requests.get(download_url.url, params={'User-agent': f'your bot {uuid.uuid4()}'})\n",
    "\n",
    "            try:\n",
    "                logger.info(f\"Started writing downloaded data into json file: {download_url.file_path}\")\n",
    "                # saving downloaded data into hard disk\n",
    "                with open(download_url.file_path, \"w\") as file_obj:\n",
    "                    finance_complaint_data = list(map(lambda x: x[\"_source\"], filter(lambda x: \"_source\" in x.keys(), json.loads(data.content))))\n",
    "                    json.dump(finance_complaint_data, file_obj)\n",
    "                logger.info(f\"Download data has been written into file: {download_url.file_path}\")\n",
    "            except Exception as e:\n",
    "                logger.info(\"Failed to download hance retry again.\")\n",
    "                # removing file failed file exists\n",
    "                if os.path.exists(download_url.file_path):\n",
    "                    os.remove(download_url.file_path)\n",
    "                self.retry_download_data(data, download_url=download_url)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.info(e)\n",
    "            raise FinanceException(e, sys)\n",
    "        \n",
    "    def retry_download_data(self, data, download_url: DownloadUrl):\n",
    "        \"\"\" \n",
    "        This function helps to avoid failure as it help to download failed file again\n",
    "\n",
    "        data: failed response\n",
    "        download_url: DownloadUrl\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # if retry still possible try else return the response\n",
    "            if download_url.n_retry == 0:\n",
    "                self.failed_download_urls.append(download_url.url)\n",
    "                logger.info(f\"Unable to download file: {download_url.url}\")\n",
    "                return\n",
    "            \n",
    "            # if in case it is not zero, to handle throatling requestion and can be solve if we wait for some second.\n",
    "            content = data.content.decode(\"utf-8\")\n",
    "            wait_second = re.findall(r'\\d+', content)\n",
    "\n",
    "            if len(wait_second) > 0:\n",
    "                time.sleep(int(wait_second[0]) + 2)\n",
    "\n",
    "            # Writing response to understand why request was failed\n",
    "            failed_file_path = os.path.join(self.data_ingestion_config.failed_dir,\n",
    "                                            os.path.basename(download_url.file_path))\n",
    "            os.makedirs(self.data_ingestion_config.failed_dir, exist_ok=True)\n",
    "            with open(failed_file_path, \"wb\") as file_obj:\n",
    "                file_obj.write(data.content)\n",
    "\n",
    "            # calling download function again to retry\n",
    "            download_url = DownloadUrl(download_url.url, file_path=download_url.file_path,\n",
    "                                       n_retry=download_url.n_retry - 1)\n",
    "            self.download_data(download_url=download_url)\n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)\n",
    "        \n",
    "    def convert_files_to_parquet(self,) -> str:\n",
    "        \"\"\" \n",
    "        downloaded files will be converted and merged into single parquet file\n",
    "        json_data_dir: downloaded json file directory\n",
    "        data_dir: convrted and combine file will be generated in data_dir\n",
    "        output_file_name: output file name\n",
    "        ========================================================================\n",
    "        returns output_file_path\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # json file download directory\n",
    "            json_data_dir = self.data_ingestion_config.download_dir\n",
    "            # feature store directoy \n",
    "            data_dir = self.data_ingestion_config.feature_store_dir\n",
    "            # aftre merging file will get a paruqet file, so its parquet file name\n",
    "            output_file_name = self.data_ingestion_config.file_name\n",
    "            #  creating features store folder\n",
    "            os.makedirs(data_dir, exist_ok=True)\n",
    "            file_path = os.path.join(data_dir, f\"{output_file_name}\")\n",
    "            logger.info(f\"Parquet file will be created at: {file_path}\")\n",
    "            # check 1st do we have json directory or not?\n",
    "            if not os.path.exists(json_data_dir):\n",
    "                return file_path\n",
    "            \n",
    "            # after listing all the files will iterate over it\n",
    "            for file_name in os.listdir(json_data_dir):\n",
    "                json_data_path = os.path.join(json_data_dir, file_name)\n",
    "                logger.debug(f\"Converting {json_data_path} into parquet format at {file_path}\")\n",
    "\n",
    "                df = spark_session.read.json(json_data_path)\n",
    "                if df.count() > 0:\n",
    "                    df.write.mode('append').parquet(file_path)\n",
    "            return file_path\n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)\n",
    "    \n",
    "    def write_metadata(self, file_path: str) -> None:\n",
    "        \"\"\" \n",
    "        This function help us to update metadata information\n",
    "        so that we can avoid redundant download and merging.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Write metadata into into metadata file\")\n",
    "            metadata_info = DataIngestionMetadata(\n",
    "                metadata_file_path=self.data_ingestion_config.metadata_file_path,\n",
    "            )\n",
    "\n",
    "            metadata_info.write_metadata_info(\n",
    "                from_date=self.data_ingestion_config.from_date,\n",
    "                to_date=self.data_ingestion_config.to_date,\n",
    "                data_file_path=file_path\n",
    "            )\n",
    "            logger.info(f\"Metadata had been written.\")\n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)\n",
    "        \n",
    "    def initiate_data_ingestion(self) -> DataIngestionArtifact:\n",
    "        \"\"\" \n",
    "        this function responsible to initiate data ingestion\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Started downloading json file\")\n",
    "\n",
    "            # check if we downloading new data or not\n",
    "            if self.data_ingestion_config.from_date != self.data_ingestion_config.to_date:\n",
    "                self.download_files()\n",
    "\n",
    "            # if json data exists start converting to parquet and update the metadata info\n",
    "            if os.path.exists(self.data_ingestion_config.download_dir):\n",
    "                logger.info(f\"Converting and combining downloaded json into parquet file.\")\n",
    "                file_path = self.convert_files_to_parquet()\n",
    "                self.write_metadata(file_path=file_path)\n",
    "\n",
    "            # path to new parquet file\n",
    "            feature_store_file_path = os.path.join(self.data_ingestion_config.feature_store_dir,\n",
    "                                                   self.data_ingestion_config.file_name)\n",
    "            \n",
    "            # store it to artifact\n",
    "            artifact = DataIngestionArtifact(\n",
    "                feature_store_file_path=feature_store_file_path,\n",
    "                download_dir=self.data_ingestion_config.download_dir,\n",
    "                metadata_file_path=self.data_ingestion_config.metadata_file_path,\n",
    "            )\n",
    "            logger.info(f\"Data ingestion artifact: {artifact}\")\n",
    "\n",
    "            return artifact\n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Testing : Data Ingestion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DataIngestion at 0x155bd16d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ingestion = DataIngestion(data_ingestion_config, 5)\n",
    "\n",
    "data_ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2016-12-31',\n",
       " '2017-12-31',\n",
       " '2018-12-31',\n",
       " '2019-12-31',\n",
       " '2020-12-31',\n",
       " '2021-12-31',\n",
       " '2022-12-31',\n",
       " '2023-12-31',\n",
       " '2024-12-31',\n",
       " '2025-04-24']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ingestion.get_required_interval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finance_complaint.exception import FinanceException\n",
    "from finance_complaint.logger import logging as logger\n",
    "# from finance_complaint.entity.config_entity import DataIngestionConfig,\n",
    "# from finance_complaint.entity.artifact_entity import DataIngestionArtifact,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingPipeline:\n",
    "\n",
    "    def __init__(self, training_pipeline_config: TrainingPipelineConfig):\n",
    "        self.training_pipeline_config: TrainingPipelineConfig = training_pipeline_config\n",
    "\n",
    "    def start_date_ingestion(self) -> DataIngestionArtifact:\n",
    "        try:\n",
    "            data_ingestion_config = DataIngestionConfig(training_pipeline_config=self.training_pipeline_config)\n",
    "            data_ingestion = DataIngestion(data_ingestion_config=data_ingestion_config)\n",
    "            data_ingestion_artifact = data_ingestion.initiate_data_ingestion()\n",
    "            return data_ingestion_artifact\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)\n",
    "        \n",
    "    def start(self):\n",
    "        try:\n",
    "            data_ingestion_artifact = self.start_date_ingestion()\n",
    "        except Exception as e:\n",
    "            raise FinanceException(e, sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **train.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m training_pipeline_config \u001b[38;5;241m=\u001b[39m TrainingPipelineConfig()\n\u001b[1;32m      3\u001b[0m training_pipeline \u001b[38;5;241m=\u001b[39m TrainingPipeline(training_pipeline_config\u001b[38;5;241m=\u001b[39mtraining_pipeline_config)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtraining_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 18\u001b[0m, in \u001b[0;36mTrainingPipeline.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m         data_ingestion_artifact \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_date_ingestion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FinanceException(e, sys)\n",
      "Cell \u001b[0;32mIn[26], line 10\u001b[0m, in \u001b[0;36mTrainingPipeline.start_date_ingestion\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      8\u001b[0m     data_ingestion_config \u001b[38;5;241m=\u001b[39m DataIngestionConfig(training_pipeline_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_pipeline_config)\n\u001b[1;32m      9\u001b[0m     data_ingestion \u001b[38;5;241m=\u001b[39m DataIngestion(data_ingestion_config\u001b[38;5;241m=\u001b[39mdata_ingestion_config)\n\u001b[0;32m---> 10\u001b[0m     data_ingestion_artifact \u001b[38;5;241m=\u001b[39m \u001b[43mdata_ingestion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_data_ingestion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data_ingestion_artifact\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[22], line 239\u001b[0m, in \u001b[0;36mDataIngestion.initiate_data_ingestion\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# check if we downloading new data or not\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_ingestion_config\u001b[38;5;241m.\u001b[39mfrom_date \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_ingestion_config\u001b[38;5;241m.\u001b[39mto_date:\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# if json data exists start converting to parquet and update the metadata info\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_ingestion_config\u001b[38;5;241m.\u001b[39mdownload_dir):\n",
      "Cell \u001b[0;32mIn[22], line 101\u001b[0m, in \u001b[0;36mDataIngestion.download_files\u001b[0;34m(self, n_day_interval_url)\u001b[0m\n\u001b[1;32m     98\u001b[0m         download_url \u001b[38;5;241m=\u001b[39m DownloadUrl(url\u001b[38;5;241m=\u001b[39murl, file_path\u001b[38;5;241m=\u001b[39mfile_path, n_retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_retry)\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;66;03m# this function downloads data from passed url\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdownload_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile download completed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \n",
      "Cell \u001b[0;32mIn[22], line 119\u001b[0m, in \u001b[0;36mDataIngestion.download_data\u001b[0;34m(self, download_url)\u001b[0m\n\u001b[1;32m    116\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(download_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# downloading data: by requesting to the url\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdownload_url\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUser-agent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myour bot \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43muuid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muuid4\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarted writing downloaded data into json file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdownload_url\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Data-Science/Data-Science-Projects/finance_complaint/venv/lib/python3.12/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Data-Science/Data-Science-Projects/finance_complaint/venv/lib/python3.12/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Data-Science/Data-Science-Projects/finance_complaint/venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/Documents/Data-Science/Data-Science-Projects/finance_complaint/venv/lib/python3.12/site-packages/requests/sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 746\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/Documents/Data-Science/Data-Science-Projects/finance_complaint/venv/lib/python3.12/site-packages/requests/models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Data-Science/Data-Science-Projects/finance_complaint/venv/lib/python3.12/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/Documents/Data-Science/Data-Science-Projects/finance_complaint/venv/lib/python3.12/site-packages/urllib3/response.py:624\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m--> 624\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Data-Science/Data-Science-Projects/finance_complaint/venv/lib/python3.12/site-packages/urllib3/response.py:828\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 828\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    830\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Data-Science/Data-Science-Projects/finance_complaint/venv/lib/python3.12/site-packages/urllib3/response.py:758\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 758\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    759\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Data-Science/Data-Science-Projects/finance_complaint/venv/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Data-Science/Data-Science-Projects/finance_complaint/venv/lib/python3.12/ssl.py:1251\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1249\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1250\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/Documents/Data-Science/Data-Science-Projects/finance_complaint/venv/lib/python3.12/ssl.py:1103\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    training_pipeline_config = TrainingPipelineConfig()\n",
    "    training_pipeline = TrainingPipeline(training_pipeline_config=training_pipeline_config)\n",
    "    training_pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- runs for around 150 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
